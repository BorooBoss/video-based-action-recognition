{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:49:23.467367690Z",
     "start_time": "2026-02-13T15:49:22.569484798Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -q transformers datasets accelerate peft pillow bitsandbytes",
   "id": "4899fe0d2646ab3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:54:59.700835147Z",
     "start_time": "2026-02-13T15:54:59.422453481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForVision2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    PaliGemmaProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from PIL import Image"
   ],
   "id": "3ff39b28b6562810",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:05:31.759719560Z",
     "start_time": "2026-02-13T16:05:31.709600994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_PATH = \"person-weapon-detection-final-1/dataset/_annotations.train.jsonl\"\n",
    "IMAGE_FOLDER = \"person-weapon-detection-final-1/dataset/\"\n",
    "MODEL_ID = \"google/paligemma2-3b-pt-224\"\n"
   ],
   "id": "729c0b09b26edbce",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:56:52.862954310Z",
     "start_time": "2026-02-13T15:56:48.683862603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = PaliGemmaProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "model.train()\n"
   ],
   "id": "25e9a6f9703c7343",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaliGemmaForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(256, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (self_attn): SiglipSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
       "    (linear): Linear(in_features=1152, out_features=2304, bias=True)\n",
       "  )\n",
       "  (language_model): Gemma2ForCausalLM(\n",
       "    (model): Gemma2Model(\n",
       "      (embed_tokens): Embedding(257216, 2304, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-25): 26 x Gemma2DecoderLayer(\n",
       "          (self_attn): Gemma2Attention(\n",
       "            (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          )\n",
       "          (mlp): Gemma2MLP(\n",
       "            (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (rotary_emb): Gemma2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2304, out_features=257216, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:05:36.485764414Z",
     "start_time": "2026-02-13T16:05:36.478032418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "{\n",
    "    \"image\": \"image1.jpg\",\n",
    "    \"prefix\": \"detect weapon\",\n",
    "    \"suffix\": \"weapon\"\n",
    "}"
   ],
   "id": "db32cb6d49ca096c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': 'image1.jpg', 'prefix': 'detect weapon', 'suffix': 'weapon'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:05:39.612154120Z",
     "start_time": "2026-02-13T16:05:39.118417243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": DATASET_PATH}\n",
    ")[\"train\"]\n",
    "\n",
    "print(dataset[0])"
   ],
   "id": "eaa9f3c85300266c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': 'aug_005426_jpg.rf.36d7b8be0b99aedc397f58929b9b1b46.jpg', 'prefix': 'detect person ; weapon', 'suffix': '<loc0425><loc0237><loc1023><loc0645> person ; <loc0227><loc0492><loc0800><loc0770> person ; <loc0209><loc0697><loc0542><loc0849> person ; <loc0822><loc0301><loc1023><loc0404> weapon'}\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:07:34.334355233Z",
     "start_time": "2026-02-13T16:07:34.272448686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    texts = []\n",
    "\n",
    "    for example in batch:\n",
    "        image_path = os.path.join(IMAGE_FOLDER, example[\"image\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # spojíme prompt + odpoveď\n",
    "        full_text = example[\"prefix\"] + \" \" + example[\"suffix\"]\n",
    "\n",
    "        images.append(image)\n",
    "        texts.append(full_text)\n",
    "\n",
    "    model_inputs = processor(\n",
    "        images=images,\n",
    "        text=texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels = model_inputs[\"input_ids\"].clone()\n",
    "\n",
    "    # nájdeme kde končí prompt a začína odpoveď\n",
    "    for i, example in enumerate(batch):\n",
    "        prompt = example[\"prefix\"]\n",
    "\n",
    "        prompt_ids = processor.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"][0]\n",
    "\n",
    "        prompt_length = len(prompt_ids)\n",
    "\n",
    "        labels[i, :prompt_length] = -100\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    return model_inputs\n"
   ],
   "id": "4f88b72b6daa224c",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:07:41.812139881Z",
     "start_time": "2026-02-13T16:07:41.793104184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./paligemma2_3d-224-weapons-ft\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ],
   "id": "19a80402a620860",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:05:51.673207221Z",
     "start_time": "2026-02-13T16:05:51.658762336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ],
   "id": "7098c234d6384bf8",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:07:45.428669727Z",
     "start_time": "2026-02-13T16:07:44.890688217Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "254241174bc034ca",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/ai_env/lib/python3.11/site-packages/transformers/trainer.py:2241\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2239\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2240\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2241\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2242\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2243\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2244\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2245\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2246\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/ai_env/lib/python3.11/site-packages/transformers/trainer.py:2270\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2268\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCurrently training with a batch size of: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m._train_batch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   2269\u001B[39m \u001B[38;5;66;03m# Data loader and number of training steps\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2270\u001B[39m train_dataloader = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_train_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2271\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_fsdp_xla_v2_enabled:\n\u001B[32m   2272\u001B[39m     train_dataloader = tpu_spmd_dataloader(train_dataloader)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/ai_env/lib/python3.11/site-packages/transformers/trainer.py:1029\u001B[39m, in \u001B[36mTrainer.get_train_dataloader\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1026\u001B[39m     dataloader_params[\u001B[33m\"\u001B[39m\u001B[33mworker_init_fn\u001B[39m\u001B[33m\"\u001B[39m] = seed_worker\n\u001B[32m   1027\u001B[39m     dataloader_params[\u001B[33m\"\u001B[39m\u001B[33mprefetch_factor\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.args.dataloader_prefetch_factor\n\u001B[32m-> \u001B[39m\u001B[32m1029\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDataLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mdataloader_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/ai_env/lib/python3.11/site-packages/accelerate/accelerator.py:1294\u001B[39m, in \u001B[36mAccelerator.prepare\u001B[39m\u001B[34m(self, device_placement, *args)\u001B[39m\n\u001B[32m   1283\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1284\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(obj, torch.nn.Module)\n\u001B[32m   1285\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verify_device_map(obj)\n\u001B[32m   1286\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.distributed_type != DistributedType.NO\n\u001B[32m   1287\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m os.environ.get(\u001B[33m\"\u001B[39m\u001B[33mACCELERATE_BYPASS_DEVICE_MAP\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfalse\u001B[39m\u001B[33m\"\u001B[39m) != \u001B[33m\"\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1288\u001B[39m     ):\n\u001B[32m   1289\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1290\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYou can\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt train a model that has been loaded with `device_map=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mauto\u001B[39m\u001B[33m'\u001B[39m\u001B[33m` in any distributed mode.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1291\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001B[39m\u001B[33m{{\u001B[39m\u001B[33mmyscript.py}}`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1292\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1294\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdistributed_type\u001B[49m == DistributedType.DEEPSPEED:\n\u001B[32m   1295\u001B[39m     model_count = \u001B[32m0\u001B[39m\n\u001B[32m   1296\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m args:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/ai_env/lib/python3.11/site-packages/accelerate/accelerator.py:568\u001B[39m, in \u001B[36mAccelerator.distributed_type\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    566\u001B[39m \u001B[38;5;129m@property\u001B[39m\n\u001B[32m    567\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdistributed_type\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m568\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdistributed_type\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/ai_env/lib/python3.11/site-packages/accelerate/state.py:1125\u001B[39m, in \u001B[36mAcceleratorState.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   1121\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m   1122\u001B[39m     \u001B[38;5;66;03m# By this point we know that no attributes of `self` contain `name`,\u001B[39;00m\n\u001B[32m   1123\u001B[39m     \u001B[38;5;66;03m# so we just modify the error message\u001B[39;00m\n\u001B[32m   1124\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._known_attrs:\n\u001B[32m-> \u001B[39m\u001B[32m1125\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[32m   1126\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m`AcceleratorState` object has no attribute `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1127\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mThis happens if `AcceleratorState._reset_state()` was called and \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1128\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33man `Accelerator` or `PartialState` was not reinitialized.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1129\u001B[39m         )\n\u001B[32m   1130\u001B[39m     \u001B[38;5;66;03m# Raise a typical AttributeError\u001B[39;00m\n\u001B[32m   1131\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[33mAcceleratorState\u001B[39m\u001B[33m'\u001B[39m\u001B[33m object has no attribute \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized."
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
